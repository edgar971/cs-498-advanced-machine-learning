
# coding: utf-8

# In[966]:

import numpy as np
import pandas as pd
import glob
from tqdm import tqdm
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from scipy.cluster.vq import vq
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

get_ipython().magic('matplotlib inline')


# # Load the data

# In[967]:

hmpLabels = ['Brush_teeth', 'Climb_stairs', 'Comb_hair', 'Descend_stairs', 'Drink_glass', 'Eat_meat', 'Eat_soup', 'Getup_bed', 'Liedown_bed',
             'Pour_water', 'Sitdown_chair', 'Standup_chair', 'Use_telephone', 'Walk']


# In[968]:

hmpData = {}

for label in hmpLabels:
    path = 'data/HMP_Dataset/' + label + '/*.txt'
    files = glob.glob(path)
    
    hmpData[label] = []
    
    for file in files:
        fileData = pd.read_csv(file, sep=" ", header=None).values
        hmpData[label].append(fileData)   


# # Segmentation

# In[969]:

def segmentData(data, segmentSize):
    segmentSize = segmentSize * 3                     # Each reading is actually 3 numbers - need to account that in segmentation
    incompleteSegmentData = len(data) % segmentSize   # We need to truncate last x rows that don't make a complete segment
    data = data[:len(data) - incompleteSegmentData]
    dataArr = np.array(data)                          # Convert to numpy array to enable easier transformation
    dataArr = np.reshape(dataArr, (-1, segmentSize))  # Do the segmentation
    return dataArr


# In[970]:

def segmentAndSplitData(trainSize, segmentSize, hmpData, hmpLabels, showOutput = True):
    if (showOutput):
        print ("Using segment size", segmentSize, "and train size", trainSize)
        print ('Segmenting classes')
    
    allTrainDataSegmented = []
    allTrainLabels = []
    allTrainFileIdentifiers = []
    allTestDataSegmented = []
    allTestLabels = []
    allTestFileIdentifiers = []
    uniqueTrainFileCtr = 0
    uniqueTestFileCtr = 0
    
    totalFilesAcrossClasses = 0
    totalTrainFilesAcrossClasses = 0
    totalTestFilesAcrossClasses = 0
    
    for hmpClass in hmpLabels: 
        totalFiles = len(hmpData[hmpClass])
        trainFiles = int(round(totalFiles * trainSize))
        testFiles = totalFiles - trainFiles
        if (showOutput):
            print (hmpClass, '| Total files:', totalFiles, '|', trainFiles, "training |", testFiles, "testing)")
        
        # Calculate totals for files across all classes
        totalFilesAcrossClasses = totalFilesAcrossClasses + totalFiles
        totalTrainFilesAcrossClasses = totalTrainFilesAcrossClasses + trainFiles
        totalTestFilesAcrossClasses = totalTestFilesAcrossClasses + testFiles
          
        # Segment training data
        trainData = []
        for i in range(trainFiles):
            trainData = hmpData[hmpClass][i].flatten()
            trainDataSegmented = segmentData(trainData, segmentSize)

            for i in range(len(trainDataSegmented)):          # Assign unique file identifier to each segment
                allTrainFileIdentifiers.append(uniqueTrainFileCtr)
                allTrainLabels.append(hmpLabels.index(hmpClass))
            uniqueTrainFileCtr = uniqueTrainFileCtr + 1
                
            allTrainDataSegmented.extend(trainDataSegmented)
                       
        # Segment test data
        testData = []
        for i in range(trainFiles, totalFiles):
            testData = hmpData[hmpClass][i].flatten()
            testDataSegmented = segmentData(testData, segmentSize)            

            for i in range(len(testDataSegmented)):          # Assign unique file identifier to each segment
                allTestFileIdentifiers.append(uniqueTestFileCtr)
                allTestLabels.append(hmpLabels.index(hmpClass))
            uniqueTestFileCtr = uniqueTestFileCtr + 1
                
            allTestDataSegmented.extend(testDataSegmented)     
    
    allTrainDataSegmentedArr = np.array(allTrainDataSegmented)
    allTestDataSegmentedArr = np.array(allTestDataSegmented)
    allTrainFileIdentifiersArr = np.array(allTrainFileIdentifiers)
    allTrainLabelsArr = np.array(allTrainLabels)
    allTestLabelsArr = np.array(allTestLabels)    
    allTestFileIdentifiersArr = np.array(allTestFileIdentifiers)    
        
    if (showOutput):
        print ('Summary for all classes:')
        print (len(allTrainDataSegmentedArr), 'total training segments |',  len(allTestDataSegmentedArr), 'total test segments')
        print ('Total files:', totalFilesAcrossClasses, '|', totalTrainFilesAcrossClasses, "training |", totalTestFilesAcrossClasses, "testing)")    
    return allTrainDataSegmentedArr, allTrainLabelsArr, allTrainFileIdentifiersArr, allTestDataSegmentedArr, allTestLabelsArr, allTestFileIdentifiersArr


# In[971]:

trainSize = 2/3
segmentSize = 1
(trainDataSegmented, trainLabels, trainFileIdentifiers, testDataSegmented, testLabels, testFileIdentifiers) = segmentAndSplitData(trainSize, segmentSize, hmpData, hmpLabels)


# # Clustering

# In[972]:

allDataSegmented = np.concatenate((trainDataSegmented, testDataSegmented), axis=0)
allFileIdentifiers = np.concatenate((trainFileIdentifiers, testFileIdentifiers), axis=None)
allLabels = np.concatenate((trainLabels, testLabels), axis=None)


# In[973]:

n_clusters = 20


# In[975]:

k_means = KMeans(n_clusters=n_clusters, random_state=0).fit(trainDataSegmented)


# In[976]:

k_means.labels_


# In[978]:

plt.scatter(trainDataSegmented[:,0],trainDataSegmented[:,1], c=k_means.labels_)  


# ## Create a histogram of cluster centers to use as an input into the classifier

# In[979]:

def createHistograms(dataSegments, labels, fileIdentifiers, model, n_clusters):
    unique, segmentCounts = np.unique(fileIdentifiers, return_counts=True)
    numFiles = len(np.unique(fileIdentifiers))
    features = np.zeros(numFiles * (n_clusters + 1), dtype = int)
    features = features.reshape(numFiles, (n_clusters + 1))
    prevSegment = 0
    
    for i in range(len(np.unique(fileIdentifiers))):
        start = prevSegment
        end = prevSegment + segmentCounts[i]
        assignment = vq(dataSegments[start:end], model.cluster_centers_) # Create vector with each segment assigned to a given cluster
        assignmentArr = np.array(assignment[0])
        feature = np.zeros(n_clusters + 1, dtype = 'int')
        assignmentArr = np.array(assignment[0])

        for j in assignmentArr:
            features[i][j] += 1
    
        features[i][n_clusters] = labels[start]
    
        prevSegment = end
    return features


# In[980]:

trainFeatures = createHistograms(trainDataSegmented, trainLabels, trainFileIdentifiers, k_means, n_clusters)
testFeatures = createHistograms(testDataSegmented, testLabels, testFileIdentifiers, k_means, n_clusters)


# In[981]:

hist = np.zeros(14 * n_clusters, dtype = int)
hist = hist.reshape(14, n_clusters)


# In[982]:

for i in range(0, 14):
    hist[i] = trainFeatures[trainFeatures[0:, 14] == i].sum(axis=0)[:-1]


# In[983]:

_, ax = plt.subplots(nrows=2, ncols=7, figsize=(24,7))
    
for i in range(0, 7):
    ax[0,i].set_title(hmpLabels[i])
    ax[0,i].hist(hist[i], normed=True, bins=100)
    
for i in range(0, 7):
    ax[1,i].set_title(hmpLabels[i+7])
    ax[1,i].hist(hist[i+7], normed=True, bins=100)


# ## Classification using Random Forest Classifier

# In[984]:

randomForestClassifier = RandomForestClassifier(max_depth=32, random_state=0, n_estimators=200)


# In[985]:

randomForestClassifier.fit(trainFeatures[0:, 0:n_clusters], trainFeatures[:, -1])


# In[986]:

prediction = randomForestClassifier.predict(testFeatures[0:, 0:n_clusters])


# In[987]:

accuracy = accuracy_score(testFeatures[:, -1], prediction)


# In[988]:

print("Classifier accuracy: " + str(round((accuracy * 100),2)) + "%")


# In[989]:

print(confusion_matrix(testFeatures[:, -1], prediction))


# # Results

# ## Page 1 (Experiment table)

# Table listing the experiments carried out with the following columns. Size of the fixed length sample Overlap (0-X%) K-value Classifier Accuracy.
# 
# We expect you to have tried at least 2 values of K and at least 2 different lengths of the windows for quantization.
# 
# Note: For K-means please also list if you used standard K-means or hierarchical.

# In[952]:

clustersToTry = [4, 8, 16, 20, 25, 30, 40, 50]
segmentSizesToTry = [1, 4, 8, 32, 64]
inertiasBySegment = {}
accPerClusterNumAndSegmentSize = []
trainSize = 2/3


# In[ ]:

for segmentSize in tqdm(segmentSizesToTry):
    inertias = []    
    for n_clusters in clustersToTry:
        (trainDataSegmented, trainLabels, trainFileIdentifiers, testDataSegmented, testLabels, testFileIdentifiers) = segmentAndSplitData(trainSize, segmentSize, hmpData, hmpLabels, False)
        k_means_segmented = KMeans(n_clusters=n_clusters, random_state=0).fit(trainDataSegmented)
        trainFeatures = createHistograms(trainDataSegmented, trainLabels, trainFileIdentifiers, k_means_segmented, n_clusters)
        
        k_means = KMeans(n_clusters=n_clusters, random_state=0).fit(trainFeatures)
        inertias.append(k_means.inertia_)
        
        trainFeatures = createHistograms(trainDataSegmented, trainLabels, trainFileIdentifiers, k_means_segmented, n_clusters)
        testFeatures = createHistograms(testDataSegmented, testLabels, testFileIdentifiers, k_means_segmented, n_clusters)
        
        # Get accuracy
        randomForestClassifier = RandomForestClassifier(max_depth = 32, random_state = 0, n_estimators=10)
        randomForestClassifier.fit(trainFeatures[0:, 0:n_clusters], trainFeatures[:, -1])
        prediction = randomForestClassifier.predict(testFeatures[0:, 0:n_clusters])
        accuracy = accuracy_score(testFeatures[:, -1], prediction)
        acc = (accuracy_score(testFeatures[:, -1], prediction))*100
        acc = round(float(acc), 4)
        accPerClusterNumAndSegmentSize.append([n_clusters, segmentSize, acc])

    inertiasBySegment[segmentSize] = inertias


# In[313]:

accPerClusterNumAndSegmentSize = np.array(accPerClusterNumAndSegmentSize)
bestAccuracy = accPerClusterNumAndSegmentSize[accPerClusterNumAndSegmentSize[:,2].argsort()][-1]
print(f'Best accuracy of {bestAccuracy[2]}% with {bestAccuracy[0]} clusters and {bestAccuracy[1]} segments')


# In[314]:

for segmentSize in inertiasBySegment:
    plt.plot(clustersToTry, inertiasBySegment[segmentSize], marker="o")
    plt.xlabel('Number of clusters')
    plt.ylabel('Average distance to cluster centers')
    plt.title('Elbow plot using segment size of ' + str(segmentSize))
    plt.grid(True)
    plt.show()


# In[315]:

xs = accPerClusterNumAndSegmentSize[:, 0]
ys = accPerClusterNumAndSegmentSize[:, 1]
zs = accPerClusterNumAndSegmentSize[:, 2]


# In[316]:

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.scatter(xs, ys, zs, c='r', marker='o')

ax.set_xlabel('Number of clusters')
ax.set_ylabel('Segment size')
ax.set_zlabel('Accuracy')

plt.show()


# ## Page 2 (Histograms)

# Histograms of the mean quantized vector (Histogram of cluster centres like in the book) for each activity with the K value that gives you the highest accuracy.
# 
# (Please state the K value)

# In[990]:

print(f'Best accuracy of {bestAccuracy[2]}% with {bestAccuracy[0]} clusters and {bestAccuracy[1]} segments')


# In[991]:

segmentSize = 1
n_clusters = 16


# In[992]:

k_means = KMeans(n_clusters=n_clusters, random_state=0).fit(trainDataSegmented)


# In[993]:

trainFeatures = createHistograms(trainDataSegmented, trainLabels, trainFileIdentifiers, k_means, n_clusters)
testFeatures = createHistograms(testDataSegmented, testLabels, testFileIdentifiers, k_means, n_clusters)


# In[994]:

hist = np.zeros(14 * n_clusters, dtype = int)
hist = hist.reshape(14, n_clusters)


# In[995]:

for i in range(0, 14):
    hist[i] = trainFeatures[trainFeatures[0:, 14] == i].sum(axis=0)[:-1]


# In[996]:

_, ax = plt.subplots(nrows=2, ncols=7, figsize=(24,7))
    
for i in range(0, 7):
    ax[0,i].set_title(hmpLabels[i])
    ax[0,i].hist(hist[i], normed=True, bins=100)
    
for i in range(0, 7):
    ax[1,i].set_title(hmpLabels[i+7])
    ax[1,i].hist(hist[i+7], normed=True, bins=100)    


# ## Page 3 (Confusion matrix)

# Class confusion matrix from the classifier that you used.
# 
# Please make sure to label the row/colums of the matrix so that we know which row corresponds to what.

# ## The average error rate over 3 fold cross validation

# In[997]:

def foldSplit(trainSection):
    trainFileIdentifiers = allFileIdentifiers[trainSection]
    trainDataSegmented = allDataSegmented[trainSection]
    trainLabels = allLabels[trainSection]
    
    testFileIdentifiers = allFileIdentifiers[~trainSection]
    testDataSegmented = allDataSegmented[~trainSection]
    testLabels = allLabels[~trainSection]
    
    return trainFileIdentifiers, trainDataSegmented, trainLabels, testFileIdentifiers, testDataSegmented, testLabels


# In[1010]:

def classifyAndReturnResults(n_clusters, trainFileIdentifiers, trainDataSegmented, trainLabels, testFileIdentifiers, testDataSegmented, testLabels):
    k_means_segmented = KMeans(n_clusters=n_clusters, random_state=0).fit(trainDataSegmented)
    
    trainFeatures = createHistograms(trainDataSegmented, trainLabels, trainFileIdentifiers, k_means_segmented, n_clusters)
    
    k_means = KMeans(n_clusters=n_clusters, random_state=0).fit(trainFeatures)
    
    trainFeatures = createHistograms(trainDataSegmented, trainLabels, trainFileIdentifiers, k_means_segmented, n_clusters)
    testFeatures = createHistograms(testDataSegmented, testLabels, testFileIdentifiers, k_means_segmented, n_clusters)
    
    randomForestClassifier = RandomForestClassifier(max_depth=32, random_state=0, n_estimators=200)
    randomForestClassifier.fit(trainFeatures[0:, 0:n_clusters], trainFeatures[:, -1])
    
    prediction = randomForestClassifier.predict(testFeatures[0:, 0:n_clusters])
    accuracy = accuracy_score(testFeatures[:, -1], prediction)
    accuracy = round((accuracy * 100),2)
    confusionMatrix = confusion_matrix(testFeatures[:, -1], prediction)
    return accuracy, confusionMatrix


# In[999]:

n_clusters = 16


# In[1000]:

allAccuracy = np.zeros(3, dtype = float)


# In[1001]:

allConfusionMatrix = np.zeros(3*14*14, dtype = int).reshape(3,14,14)


# In[1002]:

trainSize = 2/3
segmentSize = 1
(trainDataSegmented, trainLabels, trainFileIdentifiers, testDataSegmented, testLabels, testFileIdentifiers) = segmentAndSplitData(trainSize, segmentSize, hmpData, hmpLabels)


# In[1003]:

for i in range(len(np.unique(testFileIdentifiers))):
    for j in np.where(testFileIdentifiers == i)[0]:
        testFileIdentifiers[j] = i + 561


# In[1004]:

allDataSegmented = np.concatenate((trainDataSegmented, testDataSegmented), axis=0)
allFileIdentifiers = np.concatenate((trainFileIdentifiers, testFileIdentifiers), axis=None)
allLabels = np.concatenate((trainLabels, testLabels), axis=None)


# In[1005]:

uniqueFiles, counts = np.unique(allFileIdentifiers, return_counts=True)


# In[1006]:

testFoldSize = int((len(uniqueFiles) / 3))
trainFoldSize = testFoldSize * 2


# In[1007]:

# Split data for folds and classify


# In[1008]:

trainSectionCount = np.arange(0, trainFoldSize)
trainSection = np.in1d(allFileIdentifiers, trainSectionCount) == True


# In[1009]:

(trainFileIdentifiers1, trainDataSegmented1, trainLabels1, testFileIdentifiers1, testDataSegmented1, testLabels1) = foldSplit(trainSection)


# In[1011]:

(allAccuracy[0], allConfusionMatrix[0]) = classifyAndReturnResults(16, trainFileIdentifiers1, trainDataSegmented1, trainLabels1, testFileIdentifiers1, testDataSegmented1, testLabels1)


# In[1012]:

trainSectionCount = np.arange(testFoldSize, trainFoldSize)
trainSection = np.in1d(allFileIdentifiers, trainSectionCount) == True


# In[1013]:

(trainFileIdentifiers2, trainDataSegmented2, trainLabels2, testFileIdentifiers2, testDataSegmented2, testLabels2) = foldSplit(trainSection)


# In[1014]:

(allAccuracy[1], allConfusionMatrix[1]) = classifyAndReturnResults(16, trainFileIdentifiers2, trainDataSegmented2, trainLabels2, testFileIdentifiers2, testDataSegmented2, testLabels2)


# In[890]:

trainSectionCount = np.concatenate((np.arange(0, testFoldSize), np.arange(trainFoldSize, trainFoldSize + testFoldSize)), axis=None)
trainSection = np.in1d(allFileIdentifiers, trainSectionCount) == True


# In[1015]:

(trainFileIdentifiers3, trainDataSegmented3, trainLabels3, testFileIdentifiers3, testDataSegmented3, testLabels3) = foldSplit(trainSection)


# In[1016]:

(allAccuracy[2], allConfusionMatrix[2]) = classifyAndReturnResults(16, trainFileIdentifiers3, trainDataSegmented3, trainLabels3, testFileIdentifiers3, testDataSegmented3, testLabels3)


# In[ ]:




# In[1017]:

print("Classifier accuracy: ", allAccuracy)


# In[880]:

print(confusion_matrix(testFeatures1[:, -1], prediction))


# In[898]:

k_means_segmented = KMeans(n_clusters=n_clusters, random_state=0).fit(allDataSegmented)


# In[899]:

trainFeatures = createHistograms(trainDataSegmented, trainLabels, trainFileIdentifiers, k_means_segmented, n_clusters)


# In[900]:

k_means = KMeans(n_clusters=n_clusters, random_state=0).fit(trainFeatures)


# In[901]:

trainFeatures = createHistograms(trainDataSegmented, trainLabels, trainFileIdentifiers, k_means_segmented, n_clusters)
testFeatures = createHistograms(testDataSegmented, testLabels, testFileIdentifiers, k_means_segmented, n_clusters)


# In[902]:

randomForestClassifier = RandomForestClassifier(max_depth=32, random_state=0, n_estimators=200)


# In[906]:

trainFeatures[0:, 0:14]


# In[ ]:

randomForestClassifier.fit(trainFeatures[0:, 0:n_clusters], trainFeatures[:, -1])
    
prediction = randomForestClassifier.predict(testFeatures[0:, 0:n_clusters])
accuracy = accuracy_score(testFeatures[:, -1], prediction)
accuracy = round((accuracy * 100),2)
confusionMatrix = confusion_matrix(testFeatures[:, -1], prediction)
return accuracy, confusionMatrix

